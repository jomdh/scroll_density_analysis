{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Analysis of Surface Volumes using non-AI techniques.\n\n## An introduction to signal processing applied to 3D Volumes.\n\nHi!\n\nThis is Joaquin Martinez, presenting my proposal to the Vesuvian Challenge.\n\nI am Data Analyst, specialising in video and photo enhancement and analysis. That's why I took the opposite direction than the vast majority of contestants: this is a non-AI software solution for deciphering our Papyrus. \n\nI was inspired by signal-processing techniques, similar utilized in software-defined radio and astrophotography,\ntreating the information in the surface volumes as a source of the signal.\n\nThis software elegantly stacks 2D TIFF images into a 3D volume, emphasizes features of interest such as ink or text, and enables further in-depth exploration. \n\nWith its unique application of deterministic algorithms, my pipeline reduces the risk of 'hallucinations', ensuring the authenticity of the results. Often the one hallucinating would be the human reviewing the process.\n\nIdeally, if this results in a fruitful space of development, I would like to create a tool for doing this calculations and visualizations in real-time, even on the slices of the papyrus rolls before extracting the surfaces.\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Highlights of the Analysis:\n\n- Is proven that the challenge can be approached algorithmically, providing a deterministic result.\n\n- I found traces of in every single surface published in the website, which could prove that this methodology is generalised, albeit far from being perfect.\n\n- The best attept so far revealed more that 15 characters of high quality, plus an indetermined amount of shades of letters very distorted.\n\n- In this fragment is worth to highligh that most of the letters are arranged in columns and rows, even forming two words, tentatively ΠΕΙΒΑ / ΒΕΡΑΜΝ (modern versions of the corresponding greek letters).\n\n- This workflow could potentially start a break through in the ML approaches, providing quality material for model training.\n\n- This workflow is planned to be turned into an standalone application, allowing to explore volume surfaces right where the letters should be found.","metadata":{}},{"cell_type":"markdown","source":"# Results\n\nAmong other tiles, as listed at the bottom of this notebook, i found a section where the distortion was very forgiving with the characters.\n\n![](https://storage.googleapis.com/kagglesdsdata/datasets/3291362/5723497/crop_unlabelled_layers_26_45_skewed_linear_gradient_1_0.4.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20230519%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230519T161527Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=04046655c2f72cbb28720fc354adbacbe389ee13754a7fc5fc77e48f4e5f10b4c056f265c81b211a65e843d5a37d6a77b1bcdc985a7c9643f79fa0b7ef7864b970d2ff2d6224757e49e08a9a63906ccc5c741b0f3a7ed3434a83bd31cf45f1c025eeaf925351c8bcbbe49e627616d7915b21ca13ce074c6d5690339a9bbf829136978e748c819240689bccc3a2407a6b6e1d3ce5b464fa14ca43ad71eade4228b9c092b4bc2f76a5889787675d525bc2e95bc2a647e4d6414f2452b401833a07632d98b7d8c18a455ce939baed4066c88eea835d87e2387df6e03d50ffd9c4aff4d3bfeee256121d1865220b8ba3d09dbcab12713a14d7c032c8cb5f91e5aa37)\n\nI am not taught in greek characters recognition, but I this is my poor guess.\n\n![](https://storage.googleapis.com/kagglesdsdata/datasets/3291498/5723703/chars.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20230519%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230519T143720Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=25a34fbcd3703c0af87be7c41b129689ce8c2db2eac9a08da92d77a91edb6c00645d0f528972f3ddc83feb16a797354a636567fe8b055651d85b13bd577230991aff3e0e67078ff8fcdb1b6b2ceeac19d88b1551db1cbd2ac7a8a45e7944d8d6c7e64ac895f808e211842a0e7ba03635de4583c4ed26c2f5bb4ffb2e4f9305eb12d658e5cde88c64cb5b6e667442da8d53b18f060848b1c63535aacaa92f625d14e846943f0aa50c9a601840ba0329a2cb96d225d9cd9702ed9b023250b9f605202039ab363693c820d9769914dfad65db1287d4243aefe93fc022f37122c089da6267744cdd58e74b61b8f1d780436904d8b74440b1b119c012803e4e7bbc22)\n\n\nOf course there are other more examples of letters, or their shadow in this document, probably distorted letters due to the virtual unfolding proccess","metadata":{}},{"cell_type":"markdown","source":"# Why it matters that this is an algorithmic approach?\n\nBecause we can safely explain why it works","metadata":{}},{"cell_type":"markdown","source":"## The code\n\nNow let's go and surf into the code. first, let's import libraries and select our target folder, containing the volumes crafted by **@Hari-Seldon**, which are extremely well crafted.\n\nThis one in particular is found in:\n\nhttp://dl.ash2txt.org/full-scrolls/Scroll1.volpkg/paths/20230512123446/\n\n### According to the website file http://dl.ash2txt.org/full-scrolls/Scroll1.volpkg/paths/20230512123446/area_cm2.txt this surface accounts for 6.17199 cm^2, so we need to split it in half to comply with the rules of the contest, keeping the continuous sheet under 4cm^2:\n\n","metadata":{}},{"cell_type":"code","source":"# basic libraries\nimport os\nimport glob\nfrom pathlib import Path\n\n# maths and tiff\nimport numpy as np\nimport tifffile\nfrom PIL import Image\n\n# jupyter widgets\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n#plotting\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches","metadata":{"execution":{"iopub.status.busy":"2023-05-19T13:53:12.927512Z","iopub.execute_input":"2023-05-19T13:53:12.927909Z","iopub.status.idle":"2023-05-19T13:53:12.934442Z","shell.execute_reply.started":"2023-05-19T13:53:12.927879Z","shell.execute_reply":"2023-05-19T13:53:12.933284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NOTE TO JP \n\nSince I don't know if I have permission to host and share surfaces here, the code will point out to an unexisting folder. ","metadata":{}},{"cell_type":"code","source":"# Set the path to folder of TIFF files\ninput_folder_name = \"input/20230512123446/layers\"\n\n# Source: http://dl.ash2txt.org/full-scrolls/Scroll1.volpkg/paths/20230512123446/\n# Credits: @Hari-Sheldon, Vesuvian Challenge project.\n\ninput_folder = Path(input_folder_name)","metadata":{"execution":{"iopub.status.busy":"2023-05-19T13:53:12.927512Z","iopub.execute_input":"2023-05-19T13:53:12.927909Z","iopub.status.idle":"2023-05-19T13:53:12.934442Z","shell.execute_reply.started":"2023-05-19T13:53:12.927879Z","shell.execute_reply":"2023-05-19T13:53:12.933284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_tiff_volume(volume):\n    # Calculate the index of the middle row\n    mid_height = volume.shape[0] // 2\n    \n    # Split the volume into two halves\n    top_half = volume[:mid_height]\n    bottom_half = volume[mid_height:]\n    \n    return top_half, bottom_half\n\n\ndef load_tiff_volume(input_folder):\n    # List all TIFF files in the directory\n    tiff_files = sorted(input_folder.glob(\"*.tif\"))\n    \n    # Read the first file to get the shape and dtype\n    with tifffile.TiffFile(tiff_files[0]) as tif:\n        sample = tif.asarray()\n        if len(sample.shape) == 2:\n            height, width = sample.shape\n            channels = 1\n        else:\n            height, width, channels = sample.shape\n\n    num_files = len(tiff_files)\n\n    # Create an empty volume array\n    volume = np.empty((height, width, num_files, channels), dtype=sample.dtype)\n\n    # Load each TIFF file into the volume array\n    for i, file in enumerate(tiff_files):\n        with tifffile.TiffFile(file) as tif:\n            image_data = tif.asarray()\n            # If the image data is 2D, we add an extra dimension to make it 3D\n            if len(image_data.shape) == 2:\n                image_data = image_data[..., np.newaxis]\n            volume[:, :, i, :] = image_data\n    return volume","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the data\n\nHere is the starting point, loading the data in the volume folder into a Numpy 3D array. for the purpose of the contest, we are going to work only in the lower section of the shape, to comply with the rules.","metadata":{}},{"cell_type":"code","source":"# Create a volume from the source material\ndata = load_tiff_volume(input_folder)\n\n# Choose whether to split the volume or not\nshould_split = True  # Set this variable to determine if splitting should be done\n\nif should_split:\n    # Split the volume and keep only the bottom half\n    _, volume = split_tiff_volume(data)\nelse:\n    volume = data\n\n# We can void the data variable to save memory for later\ndel data","metadata":{"execution":{"iopub.status.busy":"2023-05-19T16:20:18.032645Z","iopub.execute_input":"2023-05-19T16:20:18.033360Z","iopub.status.idle":"2023-05-19T16:20:18.650991Z","shell.execute_reply.started":"2023-05-19T16:20:18.033306Z","shell.execute_reply":"2023-05-19T16:20:18.649634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The secret sauce, Analysis of differential of density.\n\nThis function computes the change in pixel values (density) between consecutive slices of our 3D volume, and returns a NEW volume of the same size where each slice contains the absolute values of these differences.\n\n**This new volume effectively represents a measure of how much each slice differs from its previous slice.**\n","metadata":{}},{"cell_type":"code","source":"def calculate_density_difference(volume):\n    num_slices = volume.shape[2]\n\n    # Create an empty volume array for the density differences\n    # The number of slices is one less than the original volume\n    volume_diff = np.empty((volume.shape[0], volume.shape[1], num_slices - 1), dtype=volume.dtype)\n\n    # Calculate the change in density for each pair of slices\n    for i in range(1, num_slices):\n        # Get the current and previous slice\n        current_slice = np.squeeze(volume[:, :, i])\n        prev_slice = np.squeeze(volume[:, :, i - 1])\n\n        # Calculate the change in density from the previous slice\n        delta_density = current_slice - prev_slice\n\n        # Add the absolute value of the change in density to the volume_diff\n        volume_diff[:, :, i - 1] = np.abs(delta_density)\n\n    return volume_diff","metadata":{"execution":{"iopub.status.busy":"2023-05-19T16:50:58.601041Z","iopub.execute_input":"2023-05-19T16:50:58.602027Z","iopub.status.idle":"2023-05-19T16:50:58.610160Z","shell.execute_reply.started":"2023-05-19T16:50:58.601985Z","shell.execute_reply":"2023-05-19T16:50:58.608785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n\nThis function applies one of the gradient transformations to a 3D volume of image data. \n\nIt processes each slice (2D image) of the volume individually:\n- First Applies an opacity threshold, setting all pixel values less than or equal to the threshold to 0. \n- Then applies the chosen gradient function to the slice. \n- The transformed slices are stacked back together to form a new volume. \n\nThis proccess is similar to the one used in medical imaging to enhance features in a series of MRI.","metadata":{}},{"cell_type":"code","source":"def process_volume(volume, gradient_function, opacity_threshold=0.0, skew=1.0):\n    num_slices = volume.shape[2]\n    processed_slices = []\n\n    for i in range(num_slices - 1):\n        # Get the slice image\n        slice_image = volume[:, :, i]\n\n        # Apply the opacity threshold\n        slice_image[slice_image <= opacity_threshold] = 0.0\n\n        # Apply the gradient function with the skew value\n        processed_slice = gradient_function(slice_image, skew=skew)\n\n        # Append the processed slice to the list\n        processed_slices.append(processed_slice)\n\n    # Stack the processed slices back together to form a new volume\n    processed_volume = np.dstack(processed_slices)\n\n    return processed_volume","metadata":{"execution":{"iopub.status.busy":"2023-05-19T16:38:31.030980Z","iopub.execute_input":"2023-05-19T16:38:31.031672Z","iopub.status.idle":"2023-05-19T16:38:31.041612Z","shell.execute_reply.started":"2023-05-19T16:38:31.031623Z","shell.execute_reply":"2023-05-19T16:38:31.039397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following functions are gradient transformations, to enhance or compress the differential density values. This could help us to fine-tune the resulting images, but as rule of thumb, a linear function will do the job.\n","metadata":{}},{"cell_type":"code","source":"def skewed_linear_gradient(image, skew=1.0):\n    normalized_image = (image - np.min(image)) / (np.max(image) - np.min(image))\n    skewed_image = normalized_image ** skew\n    return np.interp(skewed_image, (np.min(skewed_image), np.max(skewed_image)), (0, 255))\n\ndef skewed_quadratic_gradient(image, skew=1.0):\n    normalized_image = (image - np.min(image)) / (np.max(image) - np.min(image))\n    skewed_image = normalized_image ** (2 * skew)\n    return np.interp(skewed_image, (np.min(skewed_image), np.max(skewed_image)), (0, 255))\n\ndef skewed_logarithmic_gradient(image, skew=1.0):\n    normalized_image = (image - np.min(image)) / (np.max(image) - np.min(image))\n    skewed_image = np.log1p(normalized_image) ** skew\n    return np.interp(skewed_image, (np.min(skewed_image), np.max(skewed_image)), (0, 255))\n\ndef skewed_parabolic_gradient(image, skew=1.0):\n    normalized_image = (image - np.min(image)) / (np.max(image) - np.min(image))\n    skewed_image = (1 - (1 - normalized_image) ** 2) ** skew\n    return np.interp(skewed_image, (np.min(skewed_image), np.max(skewed_image)), (0, 255))","metadata":{"execution":{"iopub.status.busy":"2023-05-19T16:24:42.771403Z","iopub.execute_input":"2023-05-19T16:24:42.771851Z","iopub.status.idle":"2023-05-19T16:24:42.786174Z","shell.execute_reply.started":"2023-05-19T16:24:42.771817Z","shell.execute_reply":"2023-05-19T16:24:42.785107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Why we use gradients? \n\nLet's not forget, we are not looking to the actual data as it was obtained in the CT, this a derivate product. We are looking at densities. \n\nThis is the main reason why the ink was previously invisible, we can infere its presence only by changes.\n\nThere's only one problem, the noise tends to grow if we do this calculation.\n\nAfter calculating the density differential, we want to see precisely those areas with relative stable and local values. Ignoring the outliers will take out most of the noise.","metadata":{}},{"cell_type":"markdown","source":"# ANALYSIS TIME!!\n\nThis is the core workflow for dealing with a batch of images. The journey begins with the import and manipulation of TIFF images, with our primary objective being to modify these images to enhance the potential visibility of characters or illustrations.\n\nA lot of maths are taking place here:\n\n- Creation of a volume is the buildup of a set of slices of our papyrus. We combine these slices to construct a 3D image.\n\n- Calculation of the average density of the volume, trying to gauge the \"density\" or thickness of the material present in our 3D image.\n\nThis brings us to the most crucial step in our pipeline: we're searching for areas where the density takes a sudden shift from one slice to another.\n\nBy figuring out the density differential, we're really inspecting the variation in density from one point to another. It's relevant because the parts of the papyrus with writing (the ink) will have a higher density compared to the parts without. These changes in density can, therefore, guide us to detect and isolate the areas with text or drawings on the papyrus.\n","metadata":{}},{"cell_type":"code","source":"# Calculate the density difference volume\nvolume = calculate_density_difference(volume)\n\n# Specify the desired skew value, threshold and gradient method\nskew_value = 1\nopacity = 0.5\ngradient_method = skewed_linear_gradient\n\n# Process the volume and get the processed volume\nvolume = process_volume(volume, gradient_method, opacity_threshold=opacity ,skew=skew_value)\n\n# Options (copypasta in fuction)\n# skewed_linear_gradient\n# skewed_quadratic_gradient\n# skewed_logarithmic_gradient\n# skewed_parabolic_gradient\n\n# Density map of the PROCESSED volume\navg_dens_processed_volume = calculate_average_density(volume)","metadata":{"execution":{"iopub.status.busy":"2023-05-19T17:06:43.385731Z","iopub.execute_input":"2023-05-19T17:06:43.386317Z","iopub.status.idle":"2023-05-19T17:06:43.427324Z","shell.execute_reply.started":"2023-05-19T17:06:43.386275Z","shell.execute_reply":"2023-05-19T17:06:43.425282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Panning out to see the whole picture.\n\nWe'll create a histogram to visually spot areas with higher density. Those peaks are like little flags waving, signaling us to pay attention. They could be potential hotspots worth exploring.\n\nEven though the papyrus may look messy with all its fibers and gaps, when we zoom out and look at the bigger picture, it's actually quite uniform. The ink tends to sit on the same spot, adding extra density. So, it's reasonable to assume that we'll see an increase in density where the ink is. On the flip side, we'll also notice valleys in the histogram, which mark the end of the previous material. These valleys give us a precise clue of where to focus our gaze.\n\nBy examining the histogram and taking note of the peaks and valleys, we gain a clear understanding of how density is distributed throughout the volume. It's like having a bird's-eye view of the data. \n\nThis helps us decide which areas to explore further, as we zoom in on those regions that show significant changes in density. In other words, we're pinpointing the spots where the ink and other important elements are most likely to be hiding on the papyrus.\n\n\nConsequently, this histogram serves as a confirmation that we have sufficient material to continue our exploration.","metadata":{}},{"cell_type":"code","source":"def plot_slide_sums(slide_sums):\n    num_files = len(slide_sums)\n    \n    # Create an array for the x-axis (slide numbers)\n    slide_numbers = np.arange(num_files)\n    \n    # Create a bar chart\n    plt.figure(figsize=(10, 6))\n    plt.bar(slide_numbers, slide_sums, color='blue', alpha=0.7)\n    plt.title('Density per Slide')\n    plt.xlabel('Slide Number')\n    plt.ylabel('Sum of Pixel Values')\n    plt.grid(True)\n    plt.show()\n\ndef calculate_slide_sums(volume):\n    num_slices = volume.shape[2]\n    slide_sums = np.empty(num_slices)\n    \n    for i in range(num_slices):\n        slide = np.squeeze(volume[:, :, i])\n        slide_sums[i] = np.sum(slide)\n        \n    return slide_sums\n\n\n# Calculate the sums for each slide in the volume\nslide_sums = calculate_slide_sums(volume)\n\n# Plot the sums for each slide\nplot_slide_sums(slide_sums)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-19T17:10:50.182671Z","iopub.execute_input":"2023-05-19T17:10:50.183584Z","iopub.status.idle":"2023-05-19T17:10:50.236001Z","shell.execute_reply.started":"2023-05-19T17:10:50.183534Z","shell.execute_reply":"2023-05-19T17:10:50.234113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's introduce a new graph that offers even greater precision: the z-scores of density compared to the average signal provided by the papyrus.\n\nThis graph serves as a laser pointer, precisely indicating which areas hold the highest information content. When examining an empty papyrus, we would expect to observe a relatively flat z-score graph, as the density remains consistent throughout.\n\nBy calculating the z-scores and plotting them against the average signal, we gain a powerful tool to identify the most significant regions within the volume. Areas with high positive z-scores stand out as peaks, representing regions with a significantly higher density of INFORMATION than the average signal, suggesting the presence of relevant content, maybe ink.\n\nConversely, regions with low or negative z-scores are represented as valleys on the graph. These valleys correspond to areas with lower density compared to the average, potentially indicating regions with less relevant information.\n\nBy examining this z-score graph, we can accurately pinpoint the precise areas that contain the most valuable information within the papyrus. If there are letters somewhere, it should be there, in the red square.","metadata":{}},{"cell_type":"code","source":"def plot_z_score_histogram(z_scores):\n    num_files = len(z_scores)\n    \n    # Create an array for the x-axis (slide numbers)\n    slide_numbers = np.arange(num_files)\n    \n    # Create a bar chart\n    plt.figure(figsize=(10, 6))\n    plt.bar(slide_numbers, z_scores, color='blue', alpha=0.7)\n    \n    # Add a red horizontal line at the mean value (which should be around 0 after standardization)\n    plt.axhline(0, color='red', linestyle='--', label='Mean Value')\n\n    # Indexes of min and max z-scores\n    min_index = np.argmin(z_scores)\n    max_index = np.argmax(z_scores)\n    \n    # Coordinates and dimensions of the rectangle\n    x = min(min_index, max_index)\n    y = min(z_scores[min_index], z_scores[max_index])\n    width = abs(max_index - min_index)\n    height = abs(z_scores[max_index] - z_scores[min_index])\n\n    # Create a red rectangle from the slide with the lowest z-score to the slide with the highest z-score\n    rect = patches.Rectangle((x, y), width, height, linewidth=1, edgecolor='r', facecolor='none')\n    plt.gca().add_patch(rect)\n    \n    \n    plt.title('Z-Score of the densityy in Each Slide')\n    plt.xlabel('Slide Number')\n    plt.ylabel('Z-Score')\n    plt.grid(True)\n    \n    # Add a legend to the plot\n    plt.legend()\n    \n    plt.show()\n\ndef calculate_z_scores(volume):\n    num_slices = volume.shape[2]\n    slide_sums = np.empty(num_slices)\n    \n    for i in range(num_slices):\n        slide = np.squeeze(volume[:, :, i])\n        slide_sums[i] = np.sum(slide)\n\n    # Standardize slide_sums by subtracting the mean and dividing by the standard deviation\n    z_scores = (slide_sums - np.mean(slide_sums)) / np.std(slide_sums)\n        \n    return z_scores\n\n\n# Calculate the z-scores for each slide in the volume\nz_scores = calculate_z_scores(volume)\n\n# Plot the z-scores for each slide\nplot_z_score_histogram(z_scores)","metadata":{"execution":{"iopub.status.busy":"2023-05-19T17:12:02.397279Z","iopub.execute_input":"2023-05-19T17:12:02.398804Z","iopub.status.idle":"2023-05-19T17:12:02.456532Z","shell.execute_reply.started":"2023-05-19T17:12:02.398740Z","shell.execute_reply":"2023-05-19T17:12:02.454559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first peak from the left in the differential density map typically corresponds to the supporting material, the papyrus itself; the second one, following the valley between the peaks, generally represents the presence of ink or other significant features on the papyrus. This peak indicates regions where the density sharply increases, signifying the areas where the ink has been applied. This is our lottery ticket.\n\nEvery sample is different, and the way it was processed may affect the result and makes the sample unique. Sometimes, we may encounter very sharp peaks in the z-score graph, indicating areas with extremely high density. While these peaks may initially seem promising, it is important to note that excessively dense information could hinder its value.\n\nThat's why we included in previous steps the skewing, gradients, and thresholding of the data. These techniques help to keep the sweet spot where the info is valuable, and can help to highlights regions that otherwise are invisible.","metadata":{}},{"cell_type":"markdown","source":"# Generating our processed image\n\nArmed with the gathered information, we can provide the program with a range of slices that show promising data.\n\nUsing this selected range of slices, the program can generate a differential density map of our volume. \n\nwe can later finetune the projection, increasing the range or playing with the density parameters. Future versions of this program would ideally make this analysis interactively, to a quicker explorations.","metadata":{}},{"cell_type":"code","source":"slice_range = slice(30,35)  \navg_density = calculate_average_density(volume, slice_range)\n\n\n\n# Plot to compare before and after, in positive and inverse color mapping\nplt.figure(figsize=(10, 10))\n\nplt.subplot(1, 2, 1)\nplt.imshow(avg_density, cmap='gray')\nplt.title('Average Density - Processed Volume')\nplt.colorbar(label='Average Density')\n\n\nplt.subplot(1, 2, 2)\nplt.imshow(avg_density, cmap='binary')\nplt.title('Average Density - Processed Volume')\nplt.colorbar(label='Average Density')\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"finally, we can export our slide for further analysis. alternatively, we can export the full processed stack","metadata":{}},{"cell_type":"code","source":"# If  image is in float format\nif avg_density.dtype.kind == 'f':\n    avg_density = (255 * (avg_density - np.min(avg_density)) / np.ptp(avg_density)).astype(np.uint8)\n\n# Create an Image object from the NumPy array\nimg = Image.fromarray(avg_density)\n\n# Save the image as a TIFF file\nimg.save(\"%s_%s_%s_%s_%s_%s%s.tiff\" % (input_folder_name, slice_range.start, slice_range.stop, gradient_method.__name__ , skew_value, opacity, ('_split' if should_split else ''  ) ))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the processed slices as TIFF files in the alpha_folder\n\n# Create a folder to save the alpha slices with gradient channel\n# afn= input_folder_name + '_alpha_slices'\n# alpha_folder = Path(afn)\n# alpha_folder.mkdir(parents=True, exist_ok=True)\n# save_slices_as_tiff(volume, alpha_folder)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image adjusting\n\nFrom here, the rest is a matter of adjust levels and contrast of the resulting image in a creative suite such as Photoshop or Gimp, by adjusting B/W Levels (important), rotating and cropping the interesting areas.\n\n![](https://storage.googleapis.com/kagglesdsdata/datasets/3292485/5725300/gimp_levels.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20230519%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230519T195949Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=ae41355d209c6b615a901fbc0ca9cb1a8822c7d4e83c64cfa0510741801870ea1b9ba30f50cfbf10c9d9ab691bd0326d348d15af4f283237bc44a76b219066f14f64690bc1eb79e0034d4cf29c1e9d00eebf8260b5558bbc74e162ae4765fea763dde0a20ff1afd675360d367719c32914cc037483d2c684b073e71eab505ebbd6bfb4560f4f08b18df839ad5f351d0f30ae607a6bf325e818f6b8b97b7e225696d694a27d4883b2a872cd2f7071cc88e8cf3a60f8d81d407e59e4968ca185cef1920ed9944586b4f419640f841bb70f72ef1bfe411ad82bd0107c801d25a3340ab1e7ccbf56de92c57a87e44d6a799137042a695165abfa7185749950223539)\n\n","metadata":{}},{"cell_type":"markdown","source":"# Does it work in the training dataset provided by the contest?\n\nOh yes! Here is the result of processing our loved Ρ (Rho) from the train/1 sample.\n\nThis is a crop of the training set, let's compare the sample pre and post differential analysis, toghether with the labels for thar region: \n\nAverage Density (before analysis)\n<div> \n    <img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/3294480/5728455/density_z.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20230520%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230520T092219Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=2b13e5dd71803fd5d088419c8d91d433cb126a0698b5f981ec31c7e447bdd5364ebb7041ff4837b48aa5a2789669805ae21490ee222c96dadf5a608b86e76217a0ba62d5cf502ef6091fcbef5d4ca06f6af2eda28ac339702ee4320171681934e586ccb11086c7d7656f90ee8d36099fbb612f06ce40ad06b70d9135c561afbb4a9ae8eb4b399ce0acb8addb9a3170b833cdefbddac50756bc6fcb1fa379b0cc6645feb7e6692f44a463661e2750710645c030df5127c75fb7f5e54b3db5f2050c1686f8f439f9e648d41005fecd4abe0457adcacbfcc839a117bdea4101b9ab135cd3a4cfac675704320a4115d33811b2ea6c7077b641083f8f2f069908fad4\" width=\"200\"/>\n</div>\n\nDifferential Analysis\n<div>\n    <img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/3294480/5728496/surface_volume_23_35_skewed_linear_gradient_10_0.5.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20230520%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230520T092905Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=6a48d6ed1642d3090dee9773256aa9672d8e6a7533340e159b974142bbbab6de25b59f7c40d2191ca0b50e37c9d3e3156a611f853d2e12b780bb65eaa24df85146095d4277d1f598ee2f992bdb540a8d580b99895b1978be2a5a1659e7bf53d2f404bfe7f7facae52232c91a1150b44b52958c45963159894be1f61a7f7c217f87b1ca012d1fdc9ccf795dc7ad6b5871ef59f8d54a409503ba788ac46c8e08546fae896c7c09bbbccb6f135b4c2d8c5254043f04a3a4fab858605934959f404cb3bf21c40bc701bb7d202d575e537d8062d406719ab045f76e2a2707942e77aaa1f244b5d42fb0d56eaee9d8350fc2fd2e479ebbf17bf9f5ce2410a544a08366\" width=\"200\"/>\n</div>\n\nLabels as provided\n<div>\n    <img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/3294480/5728432/inklabels.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20230520%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230520T092130Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=a997a979ba2ab64f204c7d9733bc3024a63341853f065abfbefdde2afeda7b55f51c9e65336d3524cc221e76364020516fa93810217edb6be75aaecb9135e101ccc0473215842990dfc9a0dc75004ae9c6b815435ccb3f775f047f855236bf3a7831c06d946a09db32bd7265c25adb802c92ba38c2dc317857256addfd1801ab5f428da9df96a47118ce0d2a31b21152a39eca9f455632c75732c87849135bbdd013349d082e7ad9d93a437e70e600c77eb0e59841ed56a9356d736a9fce9e060eb20fb5abb86ee2023e2b247a189583c4e34cc4948e8980271a7ba187d15efc8f8e2100f01c0ce2a00d2764ef69aa13320d77576c7698a60ca51ee79a1e17bb\" width=\"200\"/>\n    \n</div>\n\nAs you can see, we are not only looking to the traces, they are the ACTUAL strokes of the letter. The shadow at its left is very likely the joint with the contiguous papyrus leaf, as seen in the first image.\n","metadata":{}},{"cell_type":"markdown","source":"# Other interesting images\n\nHere is another interesting candidate meeting the criteria stablished in the contest. This is the upper section of the biggest Surface extracted to date, found in:\n\nhttp://dl.ash2txt.org/full-scrolls/Scroll1.volpkg/paths/20230509182749/\n\nthanks againg to the excellent work of Hari.\n\n![](https://storage.googleapis.com/kagglesdsdata/datasets/3292534/5725377/1.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20230519%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230519T202351Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=374c42035e1fe2cdebad2bdcaadd7f7a11659539a0c86cbaa3769c8b9300577637b6334c793662660160e56bf8612fcb31bedbff0a2a714977a6f06165ed8cffbb09043ac0b994322da6bca8f6d8aaf1fba77a72164b10feca39fa283133dddf532295e1ad9c54deb2dd2925da2329ba9c971c430b1f41b215820bb9f8accad97397844328d2c43975e8b862e4bc09aacc73af361eed15dc1226354eda456f001036210e5c141cc40b72e0f1344636f9377ce0fce8bca297684e3ce7ccd8ea584e28908d7a73782e89593934ad34b3a3438358af0d89d77c2f16edb746b1089ccb4475c236bf513d241d0a54a6a11bbcd0be762db267bd30dd1551df36b29d0c)\n\nHard to spot the letters? here is a little help\n\n![](https://storage.googleapis.com/kagglesdsdata/datasets/3292534/5725377/3.gif?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20230519%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230519T202429Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=2f41bd1b5879beb56f9f7a44241431fa03d3d678427ff9211416da2162d1f2ab024fcde337e529cd50641ca3ed4c0c7746c2c70ba7b2106816fca48ef639bf6ee95f1e1b124c40b19b2e4fb4a4af6dd1c2c803912e14df7d4d2012bdda1f573e44045d5abc1917ebba8b142992025ac1d2cbf4ddc37e239521864ca18d4633b751f360b09a79e674a8f89ea05a41ca8a6fd4f34724ed2a54a284493a889b3d36a2809c92010467d849d7d14e8769e0b112124c666a77fa1aa07b4b10afa40b11097814da131928aa3da2bfc75422cf12e10d922682989b0ad0a6f4c7b2ecc22ad5c5f4e6ae4196b95a83e2371424661c88756f2c975ae57aebe8fce8823ea325)","metadata":{}}]}